{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UkJQgCyVCkZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet langchain langchain_cohere langchain-openai tiktoken langchainhub chromadb langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D41pwhu1Ce9v",
        "outputId": "57eb0d48-7e8a-4913-85e6-66eb6e59ac4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### LLMs\n",
        "import os\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = \"Your API Key here\""
      ],
      "metadata": {
        "id": "Ix6yi2wQ_U3X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Dosyayı yükle\n",
        "loader = TextLoader(\"llm-wikipedia.txt\")  # dosyanın içeriğini yükle\n",
        "documents = loader.load()  # loader nesnesinin load() metodunu çağırarak, belgeleri yükleyip documents değişkenine ata\n",
        "\n",
        "# RecursiveCharacterTextSplitter sınıfını kullanarak bir metin parçalayıcı oluştur\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)  # documentsdeki verileri text_splitter ile küçük parçalara ayırma\n",
        "\n",
        "docs[0]\n",
        "\n",
        "# Cohere Embedding modelini kullanarak embedding işlemi\n",
        "embd = CohereEmbeddings(model=\"embed-english-v2.0\")  # Burada embed modelini kullandık\n",
        "\n",
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embd,\n",
        ")\n",
        "\n",
        "# Retriever oluştur\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "rew-_gNwZUYX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## birden fazla dosya varsa\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Birden fazla dosya yüklemek için dosya yollarını liste halinde belirtin\n",
        "file_paths = [\n",
        "    \"llm-wikipedia.txt\",\n",
        "    \"llm-history.txt\",\n",
        "]\n",
        "\n",
        "# Dosyaları yükle\n",
        "documents = []\n",
        "for file_path in file_paths:\n",
        "    loader = TextLoader(file_path)\n",
        "    documents.extend(loader.load())  # Her dosyanın içeriğini yükleyip documents listesine ekle\n",
        "\n",
        "# RecursiveCharacterTextSplitter sınıfını kullanarak bir metin parçalayıcı oluştur\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "\n",
        "# Yüklenen belgeleri parçalara ayır\n",
        "docs = text_splitter.split_documents(documents)  # documentsdeki verileri text_splitter ile küçük parçalara ayırma\n",
        "\n",
        "embd = CohereEmbeddings(model=\"embed-english-v2.0\")\n",
        "\n",
        "# Vectorstore'a ekle\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embd,\n",
        ")\n",
        "\n",
        "# Retriever oluştur\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "jKewjMv1addx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_2Q7krsaUYfn",
        "outputId": "a1b9221d-f635-4655-b0b7-9c355992b284"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'llm-wikipedia.txt'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Router\n",
        "### Bu kod, bir router yapısı oluşturur. Kullanıcının soruları, belirli bir vectorstore'a yönlendirilir ve Cohere modelini kullanarak bu sorulara cevap verilir.\n",
        "### Model, sorulara verilen yanıtların doğruluğunu vectorstore'daki içeriklere dayanarak belirler.\n",
        "### Yanıtlar, tool calls aracılığıyla vectorstore'dan alınan bilgiyle desteklenir.\n",
        "\n",
        "from langchain_cohere import ChatCohere #kullanıcı ile etkileşimde bulunarak soruları yanıtlamak için kullanılır\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "# Data model\n",
        "\n",
        "class vectorstore(BaseModel):\n",
        "    \"\"\"\n",
        "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.\n",
        "    \"\"\"\n",
        "\n",
        "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
        "\n",
        "\n",
        "# Preamble\n",
        "preamble = \"\"\"You are an expert at routing a user question to a vectorstore.\n",
        "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
        "Use the vectorstore for questions on these topics.\"\"\"\n",
        "\n",
        "# LLM with tool use and preamble\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "structured_llm_router = llm.bind_tools(\n",
        "    tools=[vectorstore], preamble=preamble # modelin, kullanıcının sorularını alıp vectorstore'a yönlendirecek şekilde yapılandırıldığını belirt\n",
        ")\n",
        "\n",
        "# Prompt\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Bu işlem, question_router'ı oluşturur ve bu nesne, kullanıcının sorusuna yanıt verirken vectorstore'ı kullanarak yönlendirme yapar.\n",
        "question_router = route_prompt | structured_llm_router\n",
        "response = question_router.invoke(\n",
        "    {\"question\": \"When the start?\"}\n",
        ")\n",
        "print(response.response_metadata[\"tool_calls\"])\n",
        "response = question_router.invoke({\"question\": \"What are the types of agent memory?\"})\n",
        "print(response.response_metadata[\"tool_calls\"])\n",
        "response = question_router.invoke({\"question\": \"Hi how are you?\"})\n",
        "# response.response_metadata içinde tool_calls anahtarının bulunup bulunmadığını kontrol ediyoruz. Eğer tool_calls varsa, bu, vectorstore'ın doğru şekilde çalıştığını gösterir\n",
        "print(\"tool_calls\" in response.response_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69T5wtHaDEXn",
        "outputId": "cb9f1e79-0b1e-4edd-d91b-31aa4947530a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 'vectorstore_72rfjg2059g7', 'type': 'function', 'function': {'name': 'vectorstore', 'arguments': '{\"query\":\"start LLM\"}'}}]\n",
            "[{'id': 'vectorstore_2v0c9n3wxm3z', 'type': 'function', 'function': {'name': 'vectorstore', 'arguments': '{\"query\":\"agent memory\"}'}}]\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Prompt\n",
        "preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments.schema(), preamble=preamble)\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"types of agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "response = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "\n",
        "\n",
        "# Raw binary score'u al\n",
        "raw_binary_score = response['binary_score']\n",
        "\n",
        "binary_score = 'yes' if raw_binary_score == 'yes' else 'no'\n",
        "\n",
        "# Sonucu yazdır\n",
        "print(f\"Retrieval score: {binary_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tYcoo6Udv8N",
        "outputId": "9362c3cc-e8fd-48b0-d321-c751f883e470"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval score: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain_core.messages import HumanMessage # insan tarafından sorulan bir soruyu veya verilen bir metni modelin alabileceği formata dönüştürür\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Preamble\n",
        "# Modelin soruları yanıtlarken kullanacağı bağlantılı bilgileri (retrieved context) kullanmasını belirtiyor. Modelin doğru yanıt veremediği durumlarda \"Bilmiyorum\" demesi gerektiğini söylüyor.\n",
        "\n",
        "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Don't say emotional answer. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
        "\n",
        "# LLM\n",
        "# .bind(preamble=preamble): Bu metod, modelin preamble'ı alıp modelin nasıl davranacağına dair talimatları ekler.\n",
        "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
        "\n",
        "\n",
        "# Prompt\n",
        "def prompt(x):\n",
        "    return ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                f\"Question: {x['question']} \\nAnswer: \",\n",
        "                additional_kwargs={\"documents\": x[\"documents\"]}, #modelin soruya verdiği yanıtı oluştururken kullanacağı bağlam verilerini belirtir.\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "question = \"What is the LLM?\"  # Example question, replace with your desired question\n",
        "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6p6tsKnDVvE",
        "outputId": "f05209bf-669d-476c-8d77-83e23821200e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-1 is usually considered the first LLM, introduced in 2018.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### LLM fallback\n",
        "### Cohere'ın command-r modelini kullanarak bir fallback (yedekleme) çözümü oluşturma\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Preamble\n",
        "preamble = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\"\"\"\n",
        "\n",
        "# LLM\n",
        "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
        "\n",
        "\n",
        "# Prompt\n",
        "def prompt(x):\n",
        "    return ChatPromptTemplate.from_messages(\n",
        "        [HumanMessage(f\"Question: {x['question']} \\nAnswer: \")]\n",
        "    )\n",
        "\n",
        "\n",
        "# Chain\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "question = \"What are the types of agent memory?\"\n",
        "generation = llm_chain.invoke({\"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKC5_uVSDV59",
        "outputId": "f3dbf871-20fe-483d-81b9-624baee9efec"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent memory can be categorized into several types, including **episodic memory** (stores specific events or experiences), **semantic memory** (stores general knowledge and facts), and **procedural memory** (stores how to perform tasks or skills). Additionally, **working memory** (temporarily holds and manipulates information) and **long-term memory** (stores information for extended periods) are also crucial components. These types enable agents to learn, adapt, and make informed decisions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "# Data model for hallucination grading\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "# LLM\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "\n",
        "# Şemayı kullanarak çıkış yapılandırmak\n",
        "structured_llm_grader = llm.with_structured_output(\n",
        "    schema=GradeHallucinations.schema()\n",
        ")\n",
        "\n",
        "# Preamble\n",
        "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", preamble),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Hallucination grader'ı oluştur\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "\n",
        "# Örnek kullanım\n",
        "# `docs` ve `generation` daha önce tanımlanmalı\n",
        "response = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
        "\n",
        "# Raw binary score'u al\n",
        "raw_binary_score = response['binary_score']\n",
        "\n",
        "# Hallüsinasyon skoru: Cevabın doğruluğuna bağlı olarak \"yes\" veya \"no\" döner\n",
        "binary_score = 'yes' if raw_binary_score == 'yes' else 'no'\n",
        "\n",
        "# Sonucu yazdır\n",
        "print(f\"Hallucination score: {binary_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ4_EhAtDsMN",
        "outputId": "e1dad1e4-a80e-4d28-a5de-21aaa97321bf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hallucination score: no\n"
          ]
        }
      ]
    },
    {
      "source": [
        "### Answer Grader\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Preamble\n",
        "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer.schema(), preamble=preamble)\n",
        "\n",
        "# Prompt\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3LyJIIfmb72",
        "outputId": "5b758fdb-a920-4ae8-9808-c5413ccb421a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'binary_score': '0'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Answer Grader\n",
        "\n",
        "# Data model for grading answers\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "# Preamble\n",
        "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question.\n",
        "Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\"\"\"\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer.schema(), preamble=preamble)\n",
        "\n",
        "# Prompt\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Grading the answer by invoking the prompt and the model\n",
        "response = answer_prompt | structured_llm_grader\n",
        "result = response.invoke({\"question\": question, \"generation\": generation})\n",
        "\n",
        "\n",
        "\n",
        "# Raw binary score'u al\n",
        "raw_binary_score = result['binary_score']\n",
        "\n",
        "# Hallüsinasyon skoru: Cevabın doğruluğuna bağlı olarak \"yes\" veya \"no\" döner\n",
        "binary_score = 'yes' if raw_binary_score == 'yes' else 'no'\n",
        "\n",
        "\n",
        "# Printing the result\n",
        "print(f\"Answer Grading score: {binary_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Vg8tFgXlG0",
        "outputId": "221427fa-2ef9-4377-bedc-64d1774df307"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer Grading score: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"|\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "khyh8mwBFQAP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents from pre-loaded documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval from pre-loaded documents (not web search)\n",
        "    documents = retriever.invoke(question)  # Assuming 'retriever' has been defined\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def llm_fallback(state):\n",
        "    \"\"\"\n",
        "    Generate answer using the LLM w/o vectorstore\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---LLM Fallback---\")\n",
        "    question = state[\"question\"]\n",
        "    generation = llm_chain.invoke({\"question\": question})\n",
        "    return {\"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer using the vectorstore\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    if not isinstance(documents, list):\n",
        "        documents = [documents]\n",
        "\n",
        "    # RAG generation (Retrieve and Generate)\n",
        "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each document for relevance\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "### Edges ###\n",
        "\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to RAG (Retrieve and Generate).\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Directly use retrieve instead of web search\n",
        "    return \"retrieve\"\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered, check relevance again\n",
        "        # We will re-generate a new query if no relevant documents\n",
        "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, GENERATE---\")\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        # We have relevant documents, so proceed to generation\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\"\n"
      ],
      "metadata": {
        "id": "R-PWwHnTnBT4"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "# Define the nodes (Removed web_search, keep retrieve and other relevant nodes)\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # Retrieve from documents\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # Grade documents\n",
        "workflow.add_node(\"generate\", generate)  # RAG (Retrieve and Generate)\n",
        "workflow.add_node(\"llm_fallback\", llm_fallback)  # LLM fallback if generation fails\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"retrieve\": \"retrieve\",  # Directly using the 'retrieve' for document-based search\n",
        "        \"llm_fallback\": \"llm_fallback\",  # Fallback to llm_fallback if no suitable documents are found\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"generate\": \"generate\",  # Proceed to generation if documents are graded and valid\n",
        "    },\n",
        ")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",  # Re-generate if hallucinations are detected\n",
        "        \"not useful\": \"retrieve\",  # Fall back to documents if generation is not useful\n",
        "        \"useful\": END,  # End the process if the answer is useful\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"llm_fallback\", END)\n",
        "\n",
        "# Compile the workflow into an app\n",
        "app = workflow.compile()\n"
      ],
      "metadata": {
        "id": "n113XN0rnBWE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"Hello, how are you today?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y2YYY4FppeZ",
        "outputId": "38549feb-e895-4a63-e144-73ff29783101"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO LLM---\n",
            "---LLM Fallback---\n",
            "\"Node 'llm_fallback':\"\n",
            "'\\n---\\n'\n",
            "(\"As an AI, I don't have feelings, but I'm functioning well and ready to \"\n",
            " \"assist you. I hope you're doing well! How can I help you today?\")\n"
          ]
        }
      ]
    }
  ]
}
