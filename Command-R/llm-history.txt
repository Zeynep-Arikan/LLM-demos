Large language models are artificial neural networks (algorithms) that have gone from a recent development to widespread use within a few years. They have been instrumental in the development of ChatGPT, the next evolutionary step in artificial intelligence. Generative AI was combined with large language models to produce a smarter version of artificial intelligence.

Large language models (LLMs) are based on artificial neural networks, and recent improvements in deep learning have supported their development.

A large language model also uses semantic technology (semantics, the semantic web, and natural language processes). The history of large language models starts with the concept of semantics, developed by the French philologist, Michel Bréal, in 1883. Bréal studied the ways languages are organized, how they change as time passes, and how words connect within a language. 
Advertisement
Homepage>Data Education>Smart Data News, Articles, & Education>A Brief History of Large Language Models
A Brief History of Large Language Models
By Keith D. Foote on December 28, 2023

Large language models are artificial neural networks (algorithms) that have gone from a recent development to widespread use within a few years. They have been instrumental in the development of ChatGPT, the next evolutionary step in artificial intelligence. Generative AI was combined with large language models to produce a smarter version of artificial intelligence.

Large language models (LLMs) are based on artificial neural networks, and recent improvements in deep learning have supported their development.

A large language model also uses semantic technology (semantics, the semantic web, and natural language processes). The history of large language models starts with the concept of semantics, developed by the French philologist, Michel Bréal, in 1883. Bréal studied the ways languages are organized, how they change as time passes, and how words connect within a language. 


Currently, semantics is used for languages developed for humans, such as Dutch or Hindi, and artificial programming languages, such as Python and Java.

Natural language processing, however, is focused on translating human communications into a language understood by computers, and back again. It uses systems that can provide an understanding of human instructions, allowing computers to understand written text, recognize speech, and translate between computer and human languages.
How Natural Language Processing Was Almost Lost Before It Started
From 1906 to 1912, Ferdinand de Saussure taught Indo-European linguistics, general linguistics, and Sanskrit at the University of Geneva. During this time he developed the foundation for a highly functional model of languages as systems.

Then, in 1913, he died, before organizing and publishing his work. 

Fortunately, Albert Sechehaye and Charles Bally, two instructors who were also Saussure’s colleagues, recognized the potential of his concepts and decided they were important enough to save. The two instructors collected his notes for his future manuscript, and then made the effort to gather the notes of Saussure’s students. Based on these, they wrote Saussere’s book, titled Cours de Linguistique Générale (translated to Language as a Science, which eventually evolved into natural language processing (NLP), which was published in 1916. 

Language as a Science laid the foundation of the structuralist approach, and later, natural language processes.

The Need for Language Translation Jump-Starts Natural Language Processing
After the end of World War II (1945), the field of natural language processing received a great deal of attention. Peace talks and the desire for international trade prompted recognition of the importance of understanding one another and promoted the hopes of creating a machine that could translate languages, automatically. 

Not too surprisingly, the goal of building a language translation machine wasn’t as easy as first assumed. However, while human languages are filled with chaos and broken rules, the language of mathematics is not. Language translation machines could be adapted quite successfully to mathematics, with its unchangeable rules.

Machine Learning and the Game of Checkers
Arthur Samuel of IBM developed a computer program for playing checkers in the early 1950s. He completed a number of algorithms that allowed his checker-playing program to improve and described it as “machine learning” in 1959.

The Mark 1 Perceptron Uses Neural Networks 
In 1958, Frank Rosenblatt, of the Cornell Aeronautical Laboratory, merged Hebb’s algorithmic model of neural networks with Samuel’s work on machine learning, creating the first artificial neural network, called the Mark 1 Perceptron. Although language translation was still a goal, computers were being built primarily for mathematical purposes (much less chaotic than languages). These huge computers, built with vacuum tubes, and used as calculators, were not manufactured, but built individually, as were their software programs.

The Perceptron was also unique because it used software designed for the IBM 704, and established that similar computers could share standardized software programs. 

Unfortunately, the Mark 1 Perceptron could not recognize many kinds of basic visual patterns (such as faces), resulting in broken expectations and cuts to neural network research and machine learning
Machine Learning Carries on as a Separate Industry  
The years between 1974 and 1980 are referred to as “the first AI winter.” AI researchers had to deal with two very basic limitations: small amounts of data storage and painfully slow processing speeds. Most universities had abandoned neural network research and a schism developed between AI and machine learning. Before this schism, ML was used primarily to train artificial intelligence.

However, the machine learning industry, which included several researchers and technicians, reorganized itself into a separate field. After the schism, the ML industry shifted its focus to probability theory and statistics, while continuing to work with neural networks. ML was used to answer phones and perform a variety of automated tasks. 